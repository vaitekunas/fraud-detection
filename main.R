###############################################################################
#
# Credit card fraud detection 
# https://www.kaggle.com/dalpozz/creditcardfraud
#
# Author: Mindaugas Vaitekunas
# Blogpost: https://blog.vaitekunas.com/credit-card-fraud-detection/
#
# -----------------------------------------------------------------------------
#
# Main file
#
###############################################################################

# This is a simple credit card fraud detection project based on a kaggle dataset
# (https://www.kaggle.com/dalpozz/creditcardfraud)
#
# This dataset is extremely unbalanced (x observations in the minority class vs 
# y observations in the majority class)
#
# The main goal of this project is to contrast a naive approach (logistic regression
# on an unbalanced dataset with feature selection based on training dataset) to a
# more sophisticated approach (an ensemble of models optimized on the validation
# dataset)
#
# Model estimation and feature/model selection is fully automated using a
# the validation dataset
#
#

# Work directory
setwd("")

# Load required libraries and project files
source("src/includes.R")

###############################################################################
#
# Project options
#
###############################################################################

fraud.options <- list(
  
  # Download data
  # (default = 1)
  opt_download_data = 1,
  
  # Link to the zipped dataset
  # Kaggle will require you to log in in order to download it
  opt_data_link = "https://www.kaggle.com/dalpozz/creditcardfraud/downloads/creditcardfraud.zip",  
  
  # Data split between train and test (validation dataset will be
  # generated by cross validation). Setting test split to 0 will make performance testing impossible
  # (default = 0.25)
  opt_data_test_fraction = 0.25,
  
  # Cross validation type 
  # Choose one from c("loocv","fivefold","tenfold")
  # (default = "loocv)
  opt_cv_type = "loocv",
  
  # Random seed to be used when splitting data into train/test and 
  # sampling the minority/majority class
  # (default = 2017)
  opt_seed = 2017,
  
  # Display descriptive statistics
  # (default = 1)
  opt_descriptive_stats = 1,
  
  # Train all models (otherwise will load from models/)
  # (default = 1)
  opt_train_models = 1,
  
  # Sampling method
  # Choose one from c("undersampling", "oversampling", "SMOTE")
  # (default = oversampling)
  opt_sampling_method = "oversampling",
  
  # Choose a sampling quote based on the minority category.
  # Setting quote to 1 will take the whole minority class.
  # Setting the quote to e.g. 5 will bootstrap (or interpolate, if using SMOTE)
  # the minority class fivefold
  # (default = 1)
  opt_sampling_minority_quote = 1,
  
  # Metrics used to display model performance on the test dataset
  # Choose some from c("sensitivity","auc")
  # (default = c("sensitivity","auc"))
  opt_test_performance_metrics = c("sensitivity","auc"),
  
  # Choose models
  # Available options: c("logistic", "logistic+ridge", "logistic+lasso", "svm", "random forest", "neural net")
  # (default = c("logistic+lasso"))
  opt_models = c("logistic+ridge", "logistic+lasso", "svm", "random forest", "neural net"),
  
  # Declare model hyperparameter ranges.
  # The optimal model will be selected based on cross validation performance of opt_models_hyperparam_criteria
  # If ties exist, then the hyperparameter closer to the start of the range will be selected. 
  # It is, thus, recommended to declare hyperparameter ranges in such a way that the start
  # of the range corresponds to a less flexible (simple) model, e.g. when selecting a ridge/lasso penalty
  # for the logistic regression, the range should start with the largest positive value.
  opt_models_hyperparam_ranges = list(
    "logistic+ridge" = list(penalty = seq(10,0,-0.1)),
    "logistic+lasso" = list(penalty = seq(10,0,-0.1)),
    "svm"            = list(C = seq(1,5,0.1)),
    "random_forest"  = list(min_obs = 10:5, max_depth = 5:10, trees = 100:1000, samples = 100:1000),
    "neural_net"     = list(hidden_layers=c(c(16,4),c(8,16,8,4,2)), dropout = seq(0.5,0.1,-0.1))
  ),
  
  # Model hyperparameter selection criteria used to select hyper parameters from declared ranges
  # When breaking ties, the value closer to the start of the range is going to be selected
  # Choose one from c("sensitivity","auc")
  # (default = "sensitivity")
  opt_models_hyperparam_criteria = "sensitivity",
  
  # Create an ensemble from opt_models
  # (default = 1)
  opt_ensemble = 1,
  
  # Ensemble method
  # Available options: c("boosting","bagging","xgboost")
  # (default="boosting)
  opt_ensemble_method = "boosting",
  
  # Criteria used to average over models
  # Choose one from c("sensitivity","auc")
  # (default = "sensitivity")
  opt_ensemble_criteria = "sensitivity"
  
)

# Validate project options.
# Will abort if any setting is incorrect, but will proceed
# and use default values if some are missing
fraud.validate_options(fraud.options)

###############################################################################
#
# Get data
#
###############################################################################

fraud.data <- fraud.get_data(fraud.options)

###############################################################################
#
# Descriptive statistics
#
###############################################################################

fraud.descriptive_statistics(fraud.data)

###############################################################################
#
# Benchmark model
#
###############################################################################

fraud.benchmark_model <- fraud.data %T>%
                         fraud.train_initialization %>%
                         fraud.train_benchmark(fraud.options) %T>%
                         fraud.train_summary(fraud.options)

###############################################################################
#
# Optimized models
#
###############################################################################

fraud.models <- fraud.data %T>%
                fraud.train_initialization(fraud.options) %>%
                fraud.train(fraud.options) %T>%
                fraud.train_summary(fraud.options)
  
###############################################################################
#
# Ensemble
#
###############################################################################

fraud.ensemble_model <- fraud.models %>% 
                        fraud.ensemble

###############################################################################
#
# Test performance
#
###############################################################################

c(fraud.benchmark_model,
  fraud.models,
  fraud.ensemble_model) %>% fraud.test_performance(fraud.options)

###############################################################################
#
# Retrain on all data
#
###############################################################################

fraud.final_models <- fraud.data %>%
                      fraud.train(fraud.options, optim=F, use_all_data=T) %T>%
                      fraud.train_summary(fraud.options)

fraud.final_ensemble_model <- fraud.models %>% 
                              fraud.ensemble %T>% 
                              fraud.export_model("ensemble")

