###############################################################################
#
# Credit card fraud detection 
# https://www.kaggle.com/dalpozz/creditcardfraud
#
# Author: Mindaugas Vaitekunas
# Blogpost: https://blog.vaitekunas.com/credit-card-fraud-detection/
#
# -----------------------------------------------------------------------------
#
# Main config file
#
###############################################################################

#' Credit card fraud detection project config
#' Should be validated via 'fraud.validate_options'
#' Will use default values if some are omitted
#'
fraud.options <- list(
  
  # Download data
  # (default = 0)
  opt_download_data = 0,
  
  # Link to the zipped dataset
  # Kaggle will require you to log in in order to download it
  opt_data_link = "https://www.kaggle.com/dalpozz/creditcardfraud/downloads/creditcardfraud.zip",  
  
  # Data split between train and test (validation dataset will be
  # generated by cross validation). Setting test split to 0 will make performance testing impossible
  # (default = 0.25)
  opt_data_test_fraction = 0.25,
  
  # Cross validation type 
  # Choose one from c("loocv","fivefold","tenfold")
  # (default = "loocv)
  opt_cv_type = "loocv",
  
  # Random seed to be used when splitting data into train/test and 
  # sampling the minority/majority class
  # (default = 2017)
  opt_seed = 2017,
  
  # Display descriptive statistics
  # (default = 1)
  opt_descriptive_stats = 1,
  
  # Train all models (otherwise will load from models/)
  # (default = 1)
  opt_train_models = 1,
  
  # Sampling method
  # Choose one from c("undersampling", "oversampling", "SMOTE")
  # (default = oversampling)
  opt_sampling_method = "oversampling",
  
  # Choose a sampling quote based on the minority category.
  # Setting quote to 1 will take the whole minority class.
  # Setting the quote to e.g. 5 will bootstrap (or interpolate, if using SMOTE)
  # the minority class fivefold
  # (default = 1)
  opt_sampling_minority_quote = 1,
  
  # Metrics used to display model performance on the test dataset
  # Choose some from c("sensitivity","auc")
  # (default = c("sensitivity","auc"))
  opt_test_performance_metrics = c("sensitivity","auc"),
  
  # Choose models
  # Available options: c("logistic", "logistic+ridge", "logistic+lasso", "svm", "random forest", "neural net")
  # (default = c("logistic+lasso"))
  opt_models = c("logistic+ridge", "logistic+lasso", "svm", "random forest", "neural net"),
  
  # Declare model hyperparameter ranges.
  # The optimal model will be selected based on cross validation performance of opt_models_hyperparam_criteria
  # If ties exist, then the hyperparameter closer to the start of the range will be selected. 
  # It is, thus, recommended to declare hyperparameter ranges in such a way that the start
  # of the range corresponds to a less flexible (simple) model, e.g. when selecting a ridge/lasso penalty
  # for the logistic regression, the range should start with the largest positive value.
  opt_models_hyperparam_ranges = list(
    "logistic+ridge" = list(penalty = seq(10,0,-0.1)),
    "logistic+lasso" = list(penalty = seq(10,0,-0.1)),
    "svm"            = list(C = seq(1,5,0.1)),
    "random_forest"  = list(min_obs = 10:5, max_depth = 5:10, trees = 100:1000, samples = 100:1000),
    "neural_net"     = list(hidden_layers=c(c(16,4),c(8,16,8,4,2)), dropout = seq(0.5,0.1,-0.1))
  ),
  
  # Model hyperparameter selection criteria used to select hyper parameters from declared ranges
  # When breaking ties, the value closer to the start of the range is going to be selected
  # Choose one from c("sensitivity","auc")
  # (default = "sensitivity")
  opt_models_hyperparam_criteria = "sensitivity",
  
  # Create an ensemble from opt_models
  # (default = 1)
  opt_ensemble = 1,
  
  # Ensemble method
  # Available options: c("boosting","bagging","xgboost")
  # (default="boosting)
  opt_ensemble_method = "boosting",
  
  # Criteria used to average over models
  # Choose one from c("sensitivity","auc")
  # (default = "sensitivity")
  opt_ensemble_criteria = "sensitivity"
  
)
